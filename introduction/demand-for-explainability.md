# Demand for Explainability

### The demand for explainability arises because of the following:

#### **Trust** 

People who use this model should trust the model with its predictions and decisions. This is highly subjective and varies from individual to individual.

#### Transparency

Transparency refers to the requirement that the end-user can understand how a decision/prediction is made by the AI system. Essentially, this means that one should understand what a model predicts and if there is any bias. Correctional Offender Management Proﬁling for Alternative Sanctions \(COMPAS\), a widely used criminal risk assessment tool, found that its predictions were unreliable and racially biased. So after gaining transparency, this insight revealed a bias in the model which was a crucial breakthrough for the company.

#### Quality

Not all models are 100 percent accurate so defining quality is important. It only makes sense for people to use a highly accurate model. To identify, log and articulate sources of error and uncertainty throughout the algorithm and its data sources helps understand the expected and worst-case implications and further aid mitigation procedures.

#### Accountability 

For decisions made by the model - who is accountable? The person who uses it or the person who built it? There should be a way to identify and assign the responsibility for a decision made by an AI system.

#### Fairness

How do we ensure that algorithmic decisions do not create discriminatory or unjust impacts when comparing across different demographics \(e.g. race, sex, etc\).

To provide a complete explanation for any decision by a black-box model can be a tedious job. This has led to a new line of research – interpretability - loosely deﬁned as the science of comprehending what a model did. So, an explanation can be evaluated in two ways: interpretability and completeness.

![](../.gitbook/assets/image%20%281%29.png)



