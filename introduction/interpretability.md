# Interpretability and Explainability

A lot of human decisions are now made by Machine Learning models. Therefore, it has become highly important to be able to explain these models, just like humans explain their decisions. Take the example of applications to companies for a job or university applications for a study program - the rejection is often not explained. If we get accepted, however, we interpret that our skills and experience match the expectations of the approving personnel, but we still do not know the exact metrics that got us the approval. The explanations behind a decision are highly useful. They can help an individual to work on the required skills and improve their chances of getting that dream job, or even help to get acceptance from a prestigious university. So, for machine learning models, which are like black-box models for most of the audience, explainability is something that is needed.

To understand why explainability is important in data science, let us take an example of diabetic retinopathy - a diabetes complication that affects eyes. It is caused by damage to the blood vessels of the light-sensitive tissue at the back of the eye \(retina\). Say we use Deep Learning with Convolutional Neural Networks for classification of the normal eye from the diabetic eye. We can easily make a model that does a fairly good job with validation accuracy of 90%. Then, the question arises - what did the model see in the image to classify it, did the model look into the same diagnostic parts of the images as done by the doctors, or did it do something else. This is a very important context wherein a person can lose eyesight if he/she is misdiagnosed \(if our model says the eye is fine but the eye was damaged, someone is going to be in big trouble!\). In such cases, explainability is meant to engender trust from a model.

![](https://lh4.googleusercontent.com/FtmX_tsjJEDYDK-kTuYCqdxp38hECeibPZFDjgGUSfwU2WAlTPZRxhdKNhDLhuF-6Y8dUI0LAMxBcpbx5Lg4R3KqR54OLIwbRGyr-ZC7_sTJNeSP4H6vRx6JFmwnbL_l0v1xdiE0)

