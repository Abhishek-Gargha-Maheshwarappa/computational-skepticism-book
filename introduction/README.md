# Introduction

Over the last few years, there have been several innovations in the field of artificial intelligence and machine learning. As technology is expanding into various domains right from academics to house-cleaning robots and others, it is significantly impacting our lives. For instance, a business or finance user is now using machine learning technology to predict the number of customers that will buy a new product or whether or not an attempted activity is uncharacteristic of the account owner. This would not have been possible to do so easily 20 years ago.



Making a machine trustworthy and reliable is one of the most important goals of data science today. Models are many times used as black boxes, wherein we give a particular input, know little of what happens inside the model, and get an output. But an important question that often gets overlooked is 'Why?' In some cases, one might not care why a decision was made, it is enough to know that the predictive performance on a test dataset was good. But in other cases, knowing the ‘Why’ can help learn more about the problem,the data and the reason why a model might fail. To be able to explain why a machine learning model is working in a particular way, can be one of the most powerful assets for a machine learning engineer.



In this book, we will start with a brief introduction of what interpretability is and briefly showcase the methods of interpretability that are used today. We will then study the interpretability of two models in detail with python code. The first is a simple intrinsic model - Linear Regression, which is fairly easy to interpret in most cases, and the other is a Neural Network - which replicates the neurons in a human brain.



