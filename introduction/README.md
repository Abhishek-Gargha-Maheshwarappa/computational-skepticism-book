# Introduction

Over the last few years, there have been several innovations in the field of artificial intelligence and machine learning. As technology is expanding into various domains right from academics to house-cleaning robots and other areas - it is significantly impacting our lives. Today, a business or finance user can use machine learning technology to predict the number of customers that will buy a new product or whether or not an attempted activity is uncharacteristic of the account owner. This would not have been possible to do so easily 20 years ago.

Making a machine trustworthy and reliable is one of the most important goals of data science today. Models are many times used as black boxes, wherein we give a particular input, know little of what happens inside the model, and get an output. But an important question that often gets overlooked is 'Why?' In some cases, one might not care why a decision was made, it is enough to know that the predictive performance on a test dataset was good. But in other cases, knowing the ‘Why’ can help learn more about the problem,the data and the reason why a model might fail. To be able to explain why a machine learning model is working in a particular way, can be one of the most powerful assets for a machine learning engineer.

In this book, we will start with a brief introduction of what interpretability is and briefly showcase the methods of interpretability that are used today. We will then study the interpretability of a few models in detail with python code. Overall, our aim is to help you understand the different methods of interpretation and their implementation in python. We will not focus on explaining how to build a model as that concept is not in line with the motive of the book. 

