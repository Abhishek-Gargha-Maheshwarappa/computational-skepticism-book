# What is Model Interpretability?

Over the last few years, there have been several innovations in the field of artificial intelligence and machine learning. As technology is expanding into various domains right from shopping to academics to  and other niche domains such as house cleaning - it is significantly impacting our lives. Today, a business or finance user can use machine learning technology to predict the number of customers that will buy a new product or whether or not an attempted activity is uncharacteristic of the account owner. This would not have been possible to do so easily 20 years ago.

Making a machine trustworthy and reliable is one of the most important goals of data science today. Models are many times used as black boxes, wherein we give a particular input, know little of what happens inside the model, and get an output. But an important question that often gets overlooked is 'Why?' In some cases, one might not care why a decision was made, it is enough to know that the predictive performance on a test dataset was good. But in other cases, knowing the ‘Why’ can help learn more about the problem,the data and the reason why a model might fail. To be able to explain why a machine learning model is working in a particular way, can be one of the most powerful assets for a machine learning engineer.

This chapter helps build a baseline definition for the term "interpretability". We will explain what it means to "interpret" a model with metaphorical examples and with some example applications of machine learning.  Overall, our aim is to help you understand the meaning of interpretation and show you why it has become so important to be able to interpret a model today. We will not focus on explaining how to build a model as that concept is not in line with the motive of the book. 

